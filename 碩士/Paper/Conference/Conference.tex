\documentclass[a4paper, onecolumn, 11pt, AutoFakeBold]{article}
%%% ==================== Package setup ====================
\usepackage{listings}       % Script listing package
\usepackage{wrapfig}        % Wrap Figure or table package
\usepackage{multicol}       % Multicolumn package
\usepackage{pdfpages}       % Include pdf files
\usepackage{fancyhdr}       % Headnote and footnote
\usepackage{geometry}       % Page margin setup
\usepackage{titlesec}       % Title format setting
\usepackage{xeCJK}          % Chinese Word package
\usepackage{xCJKnumb}       % Chinese numbering
\usepackage{fontspec}       % More word font
\usepackage{indentfirst}    % Section indenting at first section
\usepackage{float}          % For figure [H] parameter place the graph
\usepackage[backend=bibtex, style=ieee]{biblatex} % Biblatex reference package(Docs: https://distrib-coffee.ipsl.jussieu.fr/pub/mirrors/ctan/macros/latex/contrib/biblatex/doc/biblatex.pdf, Cheat Sheet: https://ftp.ntou.edu.tw/ctan/info/biblatex-cheatsheet/biblatex-cheatsheet.pdf)

\usepackage{tikz}       % TikZ picture package(Docs: https://ftp.ntou.edu.tw/ctan/graphics/pgf/base/doc/pgfmanual.pdf)
% \usepackage{parskip}
\usepackage{blindtext}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{resizegather}

%%% ==================== Command setup ====================
\newcommand{\textten}{\fontsize{10}{10}\selectfont}
\newcommand{\texttwelve}{\fontsize{12}{12}\selectfont}
\newcommand{\textfourteen}{\fontsize{14}{14}\selectfont}
\newcommand{\textpt}[1]{\fontsize{#1}{#1}\selectfont}
\newcommand{\figref}[1]{圖\ref{#1}}
\newcommand{\figwidth}{2.5in}
\renewcommand{\eqref}[1]{式(\ref{#1})}

%%% ==================== Setting setup ====================
% TikZ setting
\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{arrows.meta}

% Biblatex setting
\addbibresource{refs.bib}
\ExecuteBibliographyOptions{eprint=false}
\ExecuteBibliographyOptions{url=false}

%%% ==================== Format setup ====================
% ---------- Setup chinese words encoder ----------
\XeTeXlinebreaklocale "zh"
\XeTeXlinebreakskip = 0pt plus 1pt
\titleformat{\section}{\centering\bfseries\texttwelve}{\xCJKnumber{\thesection}、}{0em}{}

% ---------- More word fonts ----------
\setmainfont{Times New Roman}
\renewcommand{\familydefault}{\rmdefault}
\setCJKmainfont{標楷體}

% ---------- Chinese paragraph format ----------
\setlength{\parindent}{2em}

% ---------- Page margin ----------
\geometry{
    left=2cm,
    right=2cm,
    top=2cm+23pt,
    bottom=2cm+23pt
}

% ---------- Rename commands ----------
\renewcommand\abstractname{\texttwelve 摘要}
\renewcommand\tablename{\textpt{11} 表}
\renewcommand\figurename{\textpt{11} 圖}
\renewcommand\refname{\texttwelve 參考文獻}

% ---------- Setup title ----------
\title{\textfourteen
    \textbf{強化學習應用於無人機姿態控制}\\
    \ \\
    \textbf{Apply reinforcement learning for UAV attitude control}
}

\author{\texttwelve
    吳柏勳$^{a}$、蕭富元$^{b}$\\
    $^{a,\ b}$淡江大學航空太空工程學系\\
    \ \\
    Wu, Po-Hsun$^{a}$, Hsiao, Fu-Yuen$^{b}$\\
    $^{a,\ b}$Department of Aerospace Engineering, Tamkung University
}

\date{}

% ---------- Setup header ----------
\pagestyle{fancy}
\renewcommand{\headrulewidth}{0pt}
\lhead{\textten
    2022 中華民國航太學會學術研討會\\
    2022 AASRC Conference\\
}
\rhead{\textpt{10}
    台中，中華民國111年11月05日\\
    Taichung, November 5th, 2022\\
    論文編號：YY-XX
}

%%% ==================== Document ====================
\begin{document}
% \setcounter{page}{1}
% ---------- Title ----------
\maketitle
\thispagestyle{fancy}

% ---------- Abstract ----------
\begin{abstract} \textpt{11}
本研究採用強化學習實現無人機的姿態控制，利用OpenAI GYM package建立強化學習的環境後，再使用強化學習演算法與Python/Tensorflow對環境進行學習。
\end{abstract}
\smallskip

% ---------- Keywords ----------
關鍵字：無人飛行載具、強化學習、OpenAI GYM、Python/Tensorflow
\smallskip

%%% ==================== Content ====================
\begin{multicols*}{2}

% ---------- Introduction ----------
\section{緒論}
\subsection{研究動機}
\par
近年來機器學習技術日漸成熟和電腦運算速度的提升，機器學習開始大量的應用於影像辨識、自然語言處理、文本分析…等領域，透過大量的訓練資料和機器學習演算法來訓練決策使其達成我們所希望達到的目標。
\par
而在控制領域通常都需要將非線性的模型線性化後，再運用PID或LQR…等方法來設計出控制器，而設計出的控制器也會因為線性化模型的緣故，在遠離平衡點時，容易與真實狀況不符合導致系統無法有效達成目標。
\par
若我們能夠利用機器學習演算法針對非線性的數學模型於電腦上進行大量模擬來訓練決策，即可得到一個以神經網路為基礎的控制器，也因為在訓練的過程中使用的模型是非線性的，所以在狀態遠離平衡點後就使控制器無法有效的達到目標。

\smallskip
\subsection{文獻回顧}
\cite{Flight_Controller_Synthesis_Via_Deep_Reinforcement_Learning}
\cite{Reinforcement_Learning_for_UAV_Attitude_Control}
\cite{Optimal_Sliding_Mode_Controller_for_Fixed-wing_UAV}

\blindtext

\smallskip
\subsection{研究方法}
\par
本研究是利用強化學習演算法來訓練決策使其可以有效的控制無人機的姿態，首先利用Python/Tensorflow建立一個神經網路來作為產生動作的決策，再來利用OpenAI GYM和四階Runge-Kutta法來進行強化學習環境的建構與無人機的動態模擬，最後利用強化學習演算法蒐集決策與環境間互動的資料進行計算，更新神經網路的參數來最佳化獎勵函數來達到控制無人機姿態的目標。

\smallskip
% ---------- Reinforcement learning ----------
\section{強化學習}
\subsection{介紹}
\par
強化學習(Reinforcement learning, RL)屬於機器學習的一種，與其他機器學習方法不同的是，強化學習是基於與環境(Environment)進行互動獲得獎勵(Reward)來改善決策(Policy)最終得到一個能夠最大化獎勵函數的決策。
\par
如\figref{fig:RL_architecture}，強化學習主要由決策、環境和演算法組成，決策會從環境中獲得狀態(State)後，根據不同的狀態反饋給環境不同的動作(Action)，而環境得到決策所給予的動作後，也會計算出下一個狀態和獎勵值給予決策，而演算法會去蒐集每個時間下的狀態、動作和獎勵值，藉由獎勵值的大小來改變決策，獎勵值大的動作會使決策增加該動作的出現機率，反之，獎勵值小的動作則會減小決策執行該動作的機率，藉由大量的學習的過程使每次的動作都能產生最大的獎勵值。
\begin{figure}[H]
    \centering
    \input{./Figs/RL_architecture.tikz}
    \caption{強化學習架構}
    \label{fig:RL_architecture}
\end{figure}

\smallskip
\subsection{PPO演算法}
\par
PPO演算法(Proximal Policy Optimization alogrithm)是由OpenAI於2017年提出的演算法，PPO是基於TRPO演算法(Trust region policy optimization)改善而來，經文獻\cite{PPO_algorithms}證實PPO演算法具有TRPO的強健性，而在文獻\cite{PPO_algorithms}的結果裡PPO演算法比起其他種強化學習的演算法具有更好的整體性能。
\par
在TRPO中，將最佳化的目標函數定義為\eqref{eqn:TRPO_optimal_max}，而約束條件定義為\eqref{eqn:TRPO_optimal_sub}
\input{Eqns/TRPO_optimal.tex}
\noindent 在\eqref{eqn:TRPO_optimal_max}中，$\hat{\mathbb{E}}_t[\,\cdot\,]$代表某個路徑上的期望值平均函數，$\hat{A}_t$則是在某時間的優勢函數表示為\eqref{eqn:Advance_function}
\input{Eqns/advance_function.tex}
\noindent 其中$r_t$為在某時間下的獎勵值；而在\eqref{eqn:TRPO_optimal_sub}中，$\text{KL}[\,\cdot\,]$是KL散度(Kullback-Leibler divergence)代表當前的決策和舊的決策之間的差異必須小於$\delta$。
\par
因為TRPO演算法中的約束條件為強約束條件，再加上KL散度難以估算，使TRPO演算法需要消耗更多的時間才能有較好的結果，所以在文獻\cite{PPO_algorithms}中提出將最佳化問題轉換為\eqref{eqn:PPO_clip}
\input{Eqns/PPO_clip.tex}
\noindent 其中$k_t(\theta)=\frac{\pi_{\theta}(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)}$，而clip函數限制了機率比值的上下限在$\left[1-\epsilon, 1+\epsilon\right]$之間，使其簡化\eqref{eqn:TRPO_optimal_sub}避免計算KL散度又可以限制新舊決策間的差異，使PPO演算法可以具有TRPO的特性又可以強化收斂時的效率。

\smallskip
\subsection{獎勵函數}
\par
在強化學習中獎勵函數的設計佔據了很重要的一部份，獎勵函數若設計的好可以使訓練出的決策有效的達成目標，而獎勵函數的目的是將決策與環境的互動狀況用數學函數來表達，好的動作則給予較高的獎勵，使演算法在更新決策時會使決策增加好的動作出現的機率。
\par
在本研究中參考了GYM預設環境的獎勵函數的定義方式，該函數是定義了狀態的上下限，若決策可以將狀態維持在某個區間內則可以獲得獎勵，反之，若決策的動作會使狀態離開該區間，則在離開該區間後則無法獲得獎勵，表示為數學形式如\eqref{eqn:Reward_function}
\input{Eqns/Reward_function.tex}
\noindent

\smallskip
% ---------- Modeling and training ----------
\section{環境與建模與訓練}
\subsection{無人機建模}
在本研究中是使用文獻\cite{Optimal_Sliding_Mode_Controller_for_Fixed-wing_UAV}中的橫向向動態方程式進行初步的驗證，如\eqref{eqn:Dynamic_model}
\input{Eqns/Dynamic_model.tex}
\noindent 其中$\beta$和$\phi$的單位為$rad$，$p$和$r$的單位為$rad/s$，而$\Delta \delta_a$和$\Delta \delta_r$的單位為$deg$，

\subsection{OpenAI GYM環境}
\par
OpenAI GYM是一個廣泛應用於建構強化學習環境的API，OpenAI GYM提供一個標準化的環境架構給開發者進行強化學習環境的建構，使在學習的期間能夠更穩定的進行模擬與學習。
\par
OpenAI GYM主要由兩個函數組成，一個是Step函數，該函數是將動作(Action)作為輸入後，經過動態方程式和獎勵函數計算後，將下一個狀態(State)、該動作的獎勵(Reward)、是否終止(Done)和其他資訊(Info)四者作為輸出，
\noindent 另一個則是Reset函數，該函數是將所有先前模擬的變數進行初始化後，生成一個隨機的初始狀態後，將該狀態進行輸出作為該次模擬的起始。

\smallskip
\subsection{訓練結果}

\smallskip
% ---------- Conclusion ----------
\section{結論}

\smallskip
% ---------- Refrence ----------
\titleformat{\section}{\bfseries\texttwelve}{}{0em}{}
\printbibliography[title={參考文獻}]

\end{multicols*}
\end{document}